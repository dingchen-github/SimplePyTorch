{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Simple Neural Network with PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch is an open source machine learning library based on the Torch \n",
    "# library, used for applications such as computer vision and natural language \n",
    "# processing, primarily developed by Facebook's AI Research lab.\n",
    "# Torch is an open-source machine learning library, a scientific computing \n",
    "# framework, and a script language based on the Lua programming language.\n",
    "import torch\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Loading and Visualizing MNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision is a library of PyTorch. It consist of popular datasets, \n",
    "# model architectures, and common image transformations for computer vision.\n",
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "# Transforms are common image transformations. \n",
    "# They can be chained together using Compose.\n",
    "# ToTensor: Converts a PIL (Python Image Library, or Pillow) Image or numpy.ndarray \n",
    "# (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) \n",
    "# in the range [0.0, 1.0] if the PIL Image belongs to one of the modes \n",
    "# (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype \n",
    "# = np.uint8.\n",
    "# Normalize: Normalize a tensor image with mean and standard deviation.\n",
    "# Mean and std are sequences for each channel.\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5),(0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: If True, creates dataset from training.pt, otherwise from test.pt.\n",
    "# Returns image and label.\n",
    "trainset = datasets.MNIST('MNIST_data/', download = False, train = True, transform = transform)\n",
    "testset = datasets.MNIST('MNIST_data/', download = False, train = False, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainset: 60000\n",
      "Size of testset: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of trainset: {}\".format(len(trainset)))\n",
    "print(\"Size of testset: {}\".format(len(testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '4')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 2\n",
    "image, label = trainset[idx]\n",
    "# image is a tensor, needs to be transformed into numpy array\n",
    "# numpy.squeeze removes axes of length one\n",
    "plt.imshow(image.numpy().squeeze(), cmap = 'gray')\n",
    "plt.title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Splitting Dataset and Setting DataLoader Into Train,Test and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training a model, we typically want to pass samples in “minibatches”, \n",
    "# reshuffle the data at every epoch to reduce model overfitting, and use \n",
    "# Python’s multiprocessing to speed up data retrieval.\n",
    "# DataLoader is an iterable that abstracts this complexity for us in an easy API.\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Split trainset into training and validation set.\n",
    "valid_size = 0.2\n",
    "num_train = len(trainset)\n",
    "split = int(np.floor(valid_size*num_train))\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples elements randomly from a given list of indices, without replacement.\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already shuffled train and validation set in the sampler.\n",
    "trainloader = DataLoader(trainset, batch_size = 64, sampler = train_sampler)\n",
    "validloader = DataLoader(trainset, batch_size = 64, sampler = valid_sampler)\n",
    "testloader = DataLoader(testset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in Trainsloader: 750\n",
      "Batches in Validloader: 188\n",
      "Batches in Testloader: 157\n",
      "Examples in Trainsloader: 48000\n",
      "Examples in Validloader: 12000\n",
      "Examples in Testloader: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Batches in Trainsloader: {}\".format(len(trainloader)))\n",
    "print(\"Batches in Validloader: {}\".format(len(validloader)))\n",
    "print(\"Batches in Testloader: {}\".format(len(testloader)))\n",
    "\n",
    "print(\"Examples in Trainsloader: {}\".format(len(trainloader.sampler)))\n",
    "print(\"Examples in Validloader: {}\".format(len(validloader.sampler)))\n",
    "print(\"Examples in Testloader: {}\".format(len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 : Creating Neural Network or Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 256)\n",
    "        self.linear2 = nn.Linear(256, 128)\n",
    "        self.linear3 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(p = 0.4)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # [64,1,28,28] -> [64,784]\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        z1 = self.linear1(images)\n",
    "        a1 = self.dropout(F.relu(z1))\n",
    "        \n",
    "        z2 = self.linear2(a1)\n",
    "        a2 = self.dropout(F.relu(z2))\n",
    "        \n",
    "        z3 = self.linear3(a2)\n",
    "        a3 = F.log_softmax(z3, dim = 1)\n",
    "        \n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (linear1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (linear3): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]         200,960\n",
      "           Dropout-2                  [-1, 256]               0\n",
      "            Linear-3                  [-1, 128]          32,896\n",
      "           Dropout-4                  [-1, 128]               0\n",
      "            Linear-5                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 235,146\n",
      "Trainable params: 235,146\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.90\n",
      "Estimated Total Size (MB): 0.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size = (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 : Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from utils import multiclass_accuracy, view_classify\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.003)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4295 Valid loss: 0.2381 Valid acc: 0.9294381737709045\n",
      "Epoch: 2 Train loss: 0.3924 Valid loss: 0.2390 Valid acc: 0.931432843208313\n",
      "Epoch: 3 Train loss: 0.3783 Valid loss: 0.2259 Valid acc: 0.931432843208313\n",
      "Epoch: 4 Train loss: 0.3661 Valid loss: 0.2099 Valid acc: 0.9384142160415649\n",
      "Epoch: 5 Train loss: 0.3582 Valid loss: 0.2405 Valid acc: 0.9250332713127136\n",
      "Epoch: 6 Train loss: 0.3440 Valid loss: 0.1996 Valid acc: 0.9408244490623474\n",
      "Epoch: 7 Train loss: 0.3355 Valid loss: 0.1847 Valid acc: 0.9473903179168701\n",
      "Epoch: 8 Train loss: 0.3362 Valid loss: 0.1992 Valid acc: 0.9438164830207825\n",
      "Epoch: 9 Train loss: 0.3319 Valid loss: 0.1963 Valid acc: 0.9432346820831299\n",
      "Epoch: 10 Train loss: 0.3220 Valid loss: 0.1831 Valid acc: 0.9473903179168701\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        # Gradient methods could work better with log probabilities.\n",
    "        logps = model(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    for images, labels in validloader:\n",
    "        logps = model(images)\n",
    "        # remove log\n",
    "        ps = torch.exp(logps)\n",
    "        loss = criterion(logps, labels)\n",
    "        valid_acc += multiclass_accuracy(ps, labels)\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    valid_loss = valid_loss / len(validloader)\n",
    "    valid_acc = valid_acc / len(validloader)\n",
    "    \n",
    "    print(\"Epoch: {} Train loss: {:.4f} Valid loss: {:.4f} Valid acc: {}\".format(e+1, train_loss, valid_loss, valid_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 : Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1686911350412733 Test acc: 0.9501393437385559\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "for images, labels in testloader:\n",
    "    logps = model(images)\n",
    "    ps = torch.exp(logps)\n",
    "    loss = criterion(logps, labels)\n",
    "    test_acc += multiclass_accuracy(ps, labels)\n",
    "    test_loss += loss.item()\n",
    "        \n",
    "test_loss = test_loss / len(testloader)\n",
    "test_acc = test_acc / len(testloader)\n",
    "\n",
    "print(\"Test loss: {} Test acc: {}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label is 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "idx = 50\n",
    "logps = model(images[idx])\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "view_classify(images[idx], ps)\n",
    "print(\"Label is {}\".format(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
